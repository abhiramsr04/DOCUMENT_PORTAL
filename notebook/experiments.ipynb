{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "703656d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pront allez!\n"
     ]
    }
   ],
   "source": [
    "print(\"Pront allez!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6b8b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb0fbf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d2c2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f0058db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq( model=\"qwen/qwen3-32b\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b924ce1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user is asking, \"What is the capital of France?\" Hmm, I need to make sure I answer this correctly. Let me start by recalling what I know about France. France is a country in Western Europe, right? I remember from geography class that the capital is Paris. But wait, maybe I should double-check to be sure. Sometimes countries have cities that are major cities but not capitals. For example, Lyon is a big city in France, but I don\\'t think it\\'s the capital. Marseille is another one, but again, not the capital. So Paris is the one. Let me think if there\\'s any other city that might be confused with the capital. Oh, maybe Versailles? But Versailles is a city known for the Palace of Versailles, which is a famous landmark. But Versailles is part of the Île-de-France region, and Paris is the capital of that region. So the capital of France is definitely Paris. I should also consider if there\\'s any other city that might be considered the capital in a different context, like a different country or administrative region, but no, Paris is the capital and largest city of France. To confirm, I can think of other European capitals. For instance, the capital of Germany is Berlin, Spain is Madrid, Italy is Rome. So Paris for France fits in that pattern. Yeah, I\\'m pretty confident that the answer is Paris. I don\\'t see any reason to doubt that. Unless there\\'s some recent change, but I don\\'t recall any news about France moving its capital. So the answer should be Paris.\\n</think>\\n\\nThe capital of France is **Paris**. It is not only the largest city in France but also a major cultural, economic, and historical center, renowned for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. Paris has served as the capital of France since the 14th century.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26cd73af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Agentic AI\\projects\\DOCUMENT_PORTAL\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e25abad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9582d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 768\n",
      "First 5 values: [0.037376475, -0.0800528, 0.009497141, -0.0542599, 0.04019698]\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Embed a query\n",
    "result = genai.embed_content(\n",
    "    model=\"embedding-001\",\n",
    "    content=\"What is the capital of France?\",\n",
    "    task_type=\"retrieval_query\",\n",
    ")\n",
    "\n",
    "embedding = result[\"embedding\"]\n",
    "print(f\"Length: {len(embedding)}\")\n",
    "print(\"First 5 values:\", embedding[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d297aa9",
   "metadata": {},
   "source": [
    "1.DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa12897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd8003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd0b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457e406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3756a39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95510625",
   "metadata": {},
   "outputs": [],
   "source": [
    "Documents = PyPDFLoader(file_path=file_path).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61bda28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be7e456e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39486",
   "metadata": {},
   "source": [
    "This is a experimental thing there is no diterministic way to dosplit the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b21818",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36900b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08eaca10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1952"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24dfe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2ebd5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_documents(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e51a0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d123ebd",
   "metadata": {},
   "source": [
    "This is the Retriever Process\n",
    "\n",
    "Means from the vectorization we are going to fetch or rank the most appropriate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d97aad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc = vector_store.similarity_search(\"llama2 finetuning benchmark experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b359902c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'descriptions of the benchmarks and metrics can be found in Appendix A.4.7. When compared toLlama 1-7B,'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c5e1c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demographic attributes.\\nWe compare the performance ofLlama 2with Llama 1(Touvron et al., 2023), Falcon (Almazrouei et al.,'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de881729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2\\nMMLU (5-shot) 70.0 86.4 69.3 78.3 68.9'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "977fc538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hyper-specialization (Scialom et al., 2020b), it is important before a newLlama 2-Chattuning iteration to'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfb4a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "997c30c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0e9ee084-686d-4a7e-b34d-567563f7bc54', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 21, 'page_label': '22'}, page_content='descriptions of the benchmarks and metrics can be found in Appendix A.4.7. When compared toLlama 1-7B,'),\n",
       " Document(id='dc33b5a1-66d3-4fe6-82f5-33be49ec2791', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 21, 'page_label': '22'}, page_content='demographic attributes.\\nWe compare the performance ofLlama 2with Llama 1(Touvron et al., 2023), Falcon (Almazrouei et al.,'),\n",
       " Document(id='f3e6dbe1-8eeb-4e85-8f1e-0d8d6b2877e6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='and PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2\\nMMLU (5-shot) 70.0 86.4 69.3 78.3 68.9'),\n",
       " Document(id='5abe339b-8877-4784-9bf3-15102792f284', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 9, 'page_label': '10'}, page_content='hyper-specialization (Scialom et al., 2020b), it is important before a newLlama 2-Chattuning iteration to')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e83b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\" Answer the question based on the context below. If you don't know the answer, just say that you \n",
    "don't know, don't try to make up an answer. \n",
    "Context: {context} \n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5cce33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14e21746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cb3685a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\" Answer the question based on the context below. If you don't know the answer, just say that you \\ndon't know, don't try to make up an answer. \\nContext: {context} \\nQuestion: {question}\\n\")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6b8b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8dcde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b886e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fefb57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8f01bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\" : retriever | format_docs, \"question\" : RunnablePassthrough()}\n",
    "     | Prompt \n",
    "     | llm \n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "905c934d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, I need to figure out how to answer the question about Llama 2 finetuning benchmark experiments based on the given context. Let me start by reading through the context carefully.\\n\\nThe context mentions that descriptions of the benchmarks and metrics can be found in Appendix A.4.7. It then talks about comparing Llama 2 to Llama 1-7B, specifically in the areas of hyper-specialization, Human-Eval, and MBPP code generation benchmarks. It also mentions evaluating Llama 2 on the NaturalQuestions and TriviaQA benchmarks, comparing it with other open-source models like Falcon and others, and Touvron et al., 2023.\\n\\nSo, the question is asking about the finetuning benchmark experiments for Llama 2. From the context, I see that Llama 2 was tuned on Human-Eval and MBPP code generation benchmarks. It was also evaluated on NaturalQuestions and TriviaQA. Comparisons were made with Llama 1 and Falcon.\\n\\nI should structure the answer by first stating that the context refers to Appendix A.4.7 for details. Then, mention the specific benchmarks used for finetuning, like Human-Eval and MBPP. Also, include the evaluation on NaturalQuestions and TriviaQA, and the models it was compared against.\\n\\nI should make sure to mention each benchmark and the comparisons clearly, without adding any information beyond what's in the context. If there's any detail missing, like specific results or other benchmarks, I might need to note that, but in this case, the context provides enough to form a coherent answer.\\n</think>\\n\\nThe context refers to Appendix A.4.7 for detailed descriptions of the benchmarks and metrics used. Llama 2 was finetuned on the Human-Eval and MBPP code generation benchmarks. Additionally, it was evaluated on the NaturalQuestions and TriviaQA benchmarks. Performance comparisons were made with other models, including Llama 1 and Falcon.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell me about llama2 finetuning benchmark experiment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3dd2de53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to figure out the answer to the question about the Granular reward model accuracy per performance rating from Table 8 based on the provided context. Let me go through this step by step.\\n\\nFirst, I\\'ll read the question again to make sure I understand it correctly. The user is asking about the Granular reward model accuracy per performance rating from Table 8. So, I\\'m looking for specific accuracy numbers related to performance ratings in Table 8.\\n\\nNow, I\\'ll look at the context provided. The context includes some text about Tables 6, 7, and 8, along with some data. Let me break it down.\\n\\nThe context starts with some numbers under \"Safety RM Meta Helpful. 64.6 57.5 53.8 52.2 56.2\" and \"Helpfulness RM 80.7 67.5 60.9 54.7 63.2\". It mentions that Table 8 reports per-preference rating accuracy for the Granular reward model. So, these numbers are likely the accuracy figures for different performance ratings.\\n\\nNext, there are mentions of other tables: Table 6 discusses reward modeling data statistics, and Table 7 reports accuracy results where their own reward models perform best. But the focus is on Table 8 for this question.\\n\\nLooking at the numbers under Safety RM, there are five values: 64.6, 57.5, 53.8, 52.2, and 56.2. Similarly, under Helpfulness RM, the numbers are 80.7, 67.5, 60.9, 54.7, and 63.2. These seem to be accuracy percentages for different performance ratings, perhaps on a scale that\\'s divided into five categories or ratings.\\n\\nI should consider what each of these numbers represents. Since they\\'re under Safety and Helpfulness, they might correspond to different aspects or models being evaluated. The numbers vary, with the highest being 80.7 under Helpfulness RM and the lowest being 52.2 under Safety RM.\\n\\nI don\\'t see any additional context that explains what each number specifically refers to, like the exact performance rating categories. Without that, it\\'s a bit challenging to provide a detailed explanation, but I can describe the numbers as they are presented.\\n\\nSo, putting it all together, the answer should list the accuracy figures from Table 8 for both Safety RM and Helpfulness RM. The user is looking for a summary of these accuracies per performance rating. I should present them clearly, indicating which model each set belongs to and the corresponding accuracy numbers.\\n</think>\\n\\nThe Granular reward model accuracy per performance rating from Table 8 is as follows:\\n\\n- **Safety RM**: The accuracy figures are 64.6, 57.5, 53.8, 52.2, and 56.2. These represent the accuracy percentages across different performance ratings for the Safety model.\\n  \\n- **Helpfulness RM**: The accuracy figures are 80.7, 67.5, 60.9, 54.7, and 63.2. These represent the accuracy percentages across different performance ratings for the Helpfulness model.\\n\\nThese numbers indicate the model\\'s performance accuracy for each respective performance rating category.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me about the Granular reward model accuracy per performance rating from table 8?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8bd539e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, I need to figure out the scaling trends for the reward model based on the given context. Let me read through the context carefully. \\n\\nThe context mentions a figure, Figure 6, which shows scaling trends for the reward model. It says that more data and a larger model size generally improve performance. They studied this by tuning different model sizes using increasing amounts of data collected each week. \\n\\nThey also talk about using diverse open-source Reward Modeling datasets. Importantly, they haven't observed any divergence yet and think that iterative model updates might be preventing it. \\n\\nSo, putting this together, the scaling trends are positive—more data and bigger models lead to better results. They haven't hit any limits yet where adding more data or size doesn't help, which is a good sign. The iterative process might be a reason why they're not seeing the usual drop-offs that sometimes happen with scaling.\\n</think>\\n\\nThe scaling trends for the reward model, as shown in Figure 6, indicate that increasing the amount of data and the size of the model generally leads to improved performance. This trend was observed when tuning different model sizes with progressively larger datasets collected each week. The study utilized diverse open-source Reward Modeling datasets, and no divergence or diminishing returns were observed, suggesting that iterative model updates may be contributing to the sustained improvements.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me about scaling trends for the reward model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0d688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ef58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57566468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463f1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b00ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
