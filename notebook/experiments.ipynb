{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "703656d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pront allez!\n"
     ]
    }
   ],
   "source": [
    "print(\"Pront allez!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6b8b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb0fbf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d2c2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f0058db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq( model=\"qwen/qwen3-32b\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b924ce1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user is asking, \"What is the capital of France?\" Hmm, I need to make sure I answer this correctly. Let me start by recalling what I know about France. France is a country in Western Europe, right? I remember from geography class that the capital is Paris. But wait, maybe I should double-check to be sure. Sometimes countries have cities that are major cities but not capitals. For example, Lyon is a big city in France, but I don\\'t think it\\'s the capital. Marseille is another one, but again, not the capital. So Paris is the one. Let me think if there\\'s any other city that might be confused with the capital. Oh, maybe Versailles? But Versailles is a city known for the Palace of Versailles, which is a famous landmark. But Versailles is part of the Île-de-France region, and Paris is the capital of that region. So the capital of France is definitely Paris. I should also consider if there\\'s any other city that might be considered the capital in a different context, like a different country or administrative region, but no, Paris is the capital and largest city of France. To confirm, I can think of other European capitals. For instance, the capital of Germany is Berlin, Spain is Madrid, Italy is Rome. So Paris for France fits in that pattern. Yeah, I\\'m pretty confident that the answer is Paris. I don\\'t see any reason to doubt that. Unless there\\'s some recent change, but I don\\'t recall any news about France moving its capital. So the answer should be Paris.\\n</think>\\n\\nThe capital of France is **Paris**. It is not only the largest city in France but also a major cultural, economic, and historical center, renowned for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. Paris has served as the capital of France since the 14th century.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26cd73af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Agentic AI\\projects\\DOCUMENT_PORTAL\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e25abad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9582d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 768\n",
      "First 5 values: [0.037376475, -0.0800528, 0.009497141, -0.0542599, 0.04019698]\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Embed a query\n",
    "result = genai.embed_content(\n",
    "    model=\"embedding-001\",\n",
    "    content=\"What is the capital of France?\",\n",
    "    task_type=\"retrieval_query\",\n",
    ")\n",
    "\n",
    "embedding = result[\"embedding\"]\n",
    "print(f\"Length: {len(embedding)}\")\n",
    "print(\"First 5 values:\", embedding[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d297aa9",
   "metadata": {},
   "source": [
    "1.DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa12897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd8003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd0b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457e406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3756a39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95510625",
   "metadata": {},
   "outputs": [],
   "source": [
    "Documents = PyPDFLoader(file_path=file_path).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61bda28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be7e456e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39486",
   "metadata": {},
   "source": [
    "This is a experimental thing there is no diterministic way to dosplit the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b21818",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36900b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08eaca10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1952"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24dfe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2ebd5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_documents(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e51a0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d123ebd",
   "metadata": {},
   "source": [
    "This is the Retriever Process\n",
    "\n",
    "Means from the vectorization we are going to fetch or rank the most appropriate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d97aad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc = vector_store.similarity_search(\"llama2 finetuning benchmark experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b359902c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'descriptions of the benchmarks and metrics can be found in Appendix A.4.7. When compared toLlama 1-7B,'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c5e1c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demographic attributes.\\nWe compare the performance ofLlama 2with Llama 1(Touvron et al., 2023), Falcon (Almazrouei et al.,'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de881729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2\\nMMLU (5-shot) 70.0 86.4 69.3 78.3 68.9'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "977fc538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hyper-specialization (Scialom et al., 2020b), it is important before a newLlama 2-Chattuning iteration to'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfb4a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "997c30c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0e9ee084-686d-4a7e-b34d-567563f7bc54', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 21, 'page_label': '22'}, page_content='descriptions of the benchmarks and metrics can be found in Appendix A.4.7. When compared toLlama 1-7B,'),\n",
       " Document(id='dc33b5a1-66d3-4fe6-82f5-33be49ec2791', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 21, 'page_label': '22'}, page_content='demographic attributes.\\nWe compare the performance ofLlama 2with Llama 1(Touvron et al., 2023), Falcon (Almazrouei et al.,'),\n",
       " Document(id='f3e6dbe1-8eeb-4e85-8f1e-0d8d6b2877e6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='and PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2\\nMMLU (5-shot) 70.0 86.4 69.3 78.3 68.9'),\n",
       " Document(id='5abe339b-8877-4784-9bf3-15102792f284', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\Agentic AI\\\\projects\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 9, 'page_label': '10'}, page_content='hyper-specialization (Scialom et al., 2020b), it is important before a newLlama 2-Chattuning iteration to')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e83b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\" Answer the question based on the context below. If you don't know the answer, just say that you \n",
    "don't know, don't try to make up an answer. \n",
    "Context: {context} \n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5cce33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14e21746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cb3685a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\" Answer the question based on the context below. If you don't know the answer, just say that you \\ndon't know, don't try to make up an answer. \\nContext: {context} \\nQuestion: {question}\\n\")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6b8b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8dcde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b886e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fefb57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8f01bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\" : retriever | format_docs, \"question\" : RunnablePassthrough()}\n",
    "     | Prompt \n",
    "     | llm \n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "905c934d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, I need to figure out how to answer the question about Llama 2 finetuning benchmark experiments based on the given context. Let me start by reading through the context carefully.\\n\\nThe context mentions that descriptions of the benchmarks and metrics can be found in Appendix A.4.7. It then talks about comparing Llama 2 to Llama 1-7B, specifically in the areas of hyper-specialization, Human-Eval, and MBPP code generation benchmarks. It also mentions evaluating Llama 2 on the NaturalQuestions and TriviaQA benchmarks, comparing it with other open-source models like Falcon and others, and Touvron et al., 2023.\\n\\nSo, the question is asking about the finetuning benchmark experiments for Llama 2. From the context, I see that Llama 2 was tuned on Human-Eval and MBPP code generation benchmarks. It was also evaluated on NaturalQuestions and TriviaQA. Comparisons were made with Llama 1 and Falcon.\\n\\nI should structure the answer by first stating that the context refers to Appendix A.4.7 for details. Then, mention the specific benchmarks used for finetuning, like Human-Eval and MBPP. Also, include the evaluation on NaturalQuestions and TriviaQA, and the models it was compared against.\\n\\nI should make sure to mention each benchmark and the comparisons clearly, without adding any information beyond what's in the context. If there's any detail missing, like specific results or other benchmarks, I might need to note that, but in this case, the context provides enough to form a coherent answer.\\n</think>\\n\\nThe context refers to Appendix A.4.7 for detailed descriptions of the benchmarks and metrics used. Llama 2 was finetuned on the Human-Eval and MBPP code generation benchmarks. Additionally, it was evaluated on the NaturalQuestions and TriviaQA benchmarks. Performance comparisons were made with other models, including Llama 1 and Falcon.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell me about llama2 finetuning benchmark experiment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2de53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd539e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
